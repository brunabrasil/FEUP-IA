{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_age                  float64\n",
       "job_type                       object\n",
       "marital                        object\n",
       "education                      object\n",
       "default                        object\n",
       "balance                       float64\n",
       "housing_loan                   object\n",
       "personal_loan                  object\n",
       "month                          object\n",
       "last_contact_duration         float64\n",
       "num_contacts_in_campaign      float64\n",
       "num_contacts_prev_campaign      int64\n",
       "prev_campaign_outcome          object\n",
       "term_deposit_subscribed         int64\n",
       "mid_month                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data = pd.read_csv('train-clean.csv',delimiter=\",\")\n",
    "# GETTING THE NULL VALUES\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = list(train_data.columns)\n",
    "input_names.remove('term_deposit_subscribed')\n",
    "\n",
    "all_inputs = train_data[input_names].values\n",
    "all_labels = train_data['term_deposit_subscribed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\"\"\" categorical_features = ['job_type', 'marital', 'education', 'default', 'housing_loan', 'personal_loan', 'month', 'prev_campaign_outcome']\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "all_inputs_encoded = pd.DataFrame(encoder.fit_transform(all_inputs[categorical_features]).toarray())\n",
    "all_inputs_encoded.columns = encoder.get_feature_names(categorical_features)\n",
    "all_inputs_encoded.index = all_inputs.index\n",
    "all_inputs_encoded = pd.concat([all_inputs.drop(categorical_features, axis=1), all_inputs_encoded], axis=1) \"\"\"\n",
    "\n",
    "all_inputs_df = pd.DataFrame(all_inputs, columns=input_names)\n",
    "categorical_features = ['job_type', 'marital', 'education', 'default', 'housing_loan', 'personal_loan', 'month', 'prev_campaign_outcome']\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_feature_indices = [all_inputs_df.columns.get_loc(col) for col in categorical_features]\n",
    "all_inputs_encoded = pd.DataFrame(encoder.fit_transform(all_inputs_df.iloc[:, categorical_feature_indices]).toarray())\n",
    "all_inputs_encoded.columns = encoder.get_feature_names(categorical_features)\n",
    "all_inputs_encoded.index = all_inputs_df.index\n",
    "all_inputs_encoded = pd.concat([all_inputs_df.drop(categorical_features, axis=1), all_inputs_encoded], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs_encoded.values, all_labels, test_size=0.25, random_state=1)\n",
    "# Split the encoded dataset into training and testing sets\n",
    "#(training_inputs, \n",
    "# testing_inputs,\n",
    "# training_classes,\n",
    "# testing_classes) = train_test_split(all_inputs_encoded.values, all_labels.values, test_size=0.25, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69280058, -0.31957411,  0.92288898, ..., -0.18931703,\n",
       "        -0.17624758, -2.22335309],\n",
       "       [ 1.18836903,  0.71799145, -0.52895821, ..., -0.18931703,\n",
       "        -0.17624758,  0.44977112],\n",
       "       [ 1.37648599,  4.29530713,  1.1735336 , ..., -0.18931703,\n",
       "        -0.17624758,  0.44977112],\n",
       "       ...,\n",
       "       [-0.69280058, -0.62702414, -0.44856277, ..., -0.18931703,\n",
       "        -0.17624758, -2.22335309],\n",
       "       [-2.10367779,  0.10121289,  1.14988788, ..., -0.18931703,\n",
       "        -0.17624758, -2.22335309],\n",
       "       [-0.22250818, -0.40097636, -0.24048043, ..., -0.18931703,\n",
       "        -0.17624758,  0.44977112]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(training_inputs)\n",
    "training_inputs = scaler.transform(training_inputs)\n",
    "testing_inputs = scaler.transform(testing_inputs)  \n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#training_inputs[:, :-32] = scaler.fit_transform(training_inputs[:, :-32])\n",
    "#testing_inputs[:, :-32] = scaler.transform(testing_inputs[:, :-32])\n",
    "\n",
    "training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034643297275092"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the classifier\n",
    "clf = MLPClassifier(max_iter=700)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "clf.fit(training_inputs, training_classes)\n",
    "\n",
    "# Validate the classifier on the testing set using classification accuracy\n",
    "clf.score(testing_inputs, testing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      6602\n",
      "           1       0.50      0.51      0.50       701\n",
      "\n",
      "    accuracy                           0.90      7303\n",
      "   macro avg       0.72      0.73      0.72      7303\n",
      "weighted avg       0.90      0.90      0.90      7303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predictions = clf.predict(testing_inputs)\n",
    "print(classification_report(testing_classes, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      6602\n",
      "           1       0.51      0.49      0.49       701\n",
      "\n",
      "    accuracy                           0.90      7303\n",
      "   macro avg       0.73      0.72      0.72      7303\n",
      "weighted avg       0.90      0.90      0.90      7303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "improved_clf = MLPClassifier(activation='logistic', alpha=0.001, hidden_layer_sizes=50, max_iter=700)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "improved_clf.fit(training_inputs, training_classes)\n",
    "new_predictions = improved_clf.predict(testing_inputs)\n",
    "print(classification_report(testing_classes, new_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Average score: 0.9082163642588155')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(clf, all_inputs_encoded, all_labels, cv=10)\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   3.3s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.908, total=   3.6s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.898, total=   3.9s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   3.3s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.900, total=   3.6s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.893, total=   5.3s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.907, total=   6.9s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.907, total=   4.8s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.904, total=   5.9s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.911, total=   8.9s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.910, total=   6.9s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.908, total=   6.5s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.909, total=   5.1s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.912, total=   9.0s\n",
      "[CV] activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.904, total=   5.3s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   3.6s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.904, total=   6.2s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.894, total=   6.8s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.900, total=   4.6s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.902, total=   4.7s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.904, total=   4.3s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.910, total=   5.8s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.909, total=   4.7s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.912, total=   7.5s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.903, total=   5.3s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.915, total=  10.6s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.912, total=   5.1s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.907, total=   7.0s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.902, total=  12.3s\n",
      "[CV] activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiagoncalves/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.905, total=   8.5s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   7.7s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   6.3s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   7.2s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.880, total=   6.6s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.900, total=   5.5s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.901, total=   7.4s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.903, total=   6.4s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.908, total=   6.8s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.900, total=   8.2s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.909, total=   4.2s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.911, total=   5.4s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.910, total=   6.5s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.908, total=   6.2s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.910, total=   6.4s\n",
      "[CV] activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=tanh, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.907, total=   7.3s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.913, total=   4.8s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.914, total=   4.9s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.915, total=   8.6s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.915, total=   5.8s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.915, total=   4.8s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.912, total=   4.3s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.918, total=   4.6s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.916, total=   3.9s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.915, total=   4.1s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.911, total=   4.5s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.918, total=   7.5s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.915, total=   8.0s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.911, total=   8.0s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.911, total=   6.9s\n",
      "[CV] activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.0001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.912, total=   5.9s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.915, total=   3.6s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.919, total=   3.6s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.913, total=   5.2s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   1.5s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.912, total=   3.1s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.908, total=   6.6s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.914, total=   4.7s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.912, total=   4.9s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.913, total=   5.0s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.910, total=   4.7s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.915, total=   7.5s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.919, total=   6.3s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.912, total=   5.0s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.910, total=   7.3s\n",
      "[CV] activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.001, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.913, total=  11.7s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.911, total=   3.7s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.911, total=   4.1s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.910, total=   4.1s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.912, total=   5.6s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=10, max_iter=500, solver=adam, score=0.901, total=   3.6s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.912, total=   3.9s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.912, total=   3.6s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.910, total=   4.8s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.911, total=   5.5s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=50, max_iter=500, solver=adam, score=0.911, total=   3.4s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.915, total=   3.7s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.919, total=   5.5s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.912, total=   6.1s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.913, total=   4.8s\n",
      "[CV] activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam \n",
      "[CV]  activation=logistic, alpha=0.01, hidden_layer_sizes=100, max_iter=500, solver=adam, score=0.910, total=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.9142759328996919\n",
      "Best parameters: {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': 50, 'max_iter': 500, 'solver': 'adam'}\n",
      "Best estimator: MLPClassifier(activation='logistic', hidden_layer_sizes=50, max_iter=500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "parameter_grid = {\n",
    "    'solver': ['adam'],\n",
    "    'activation' : ['tanh', 'logistic'],\n",
    "    'alpha': [0.0001, 0.001, 0.01], \n",
    "    'hidden_layer_sizes': [10, 50, 100],\n",
    "    'max_iter' : [700]}\n",
    "\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=5)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    MLPClassifier(), \n",
    "    param_grid=parameter_grid, \n",
    "    cv=cross_validation,\n",
    "    verbose=3)\n",
    "\n",
    "grid_search.fit(all_inputs_encoded, all_labels)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best estimator: {}'.format(grid_search.best_estimator_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
